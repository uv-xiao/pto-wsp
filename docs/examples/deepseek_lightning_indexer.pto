// ============================================================================
// DeepSeek Lightning Indexer - v3 Plan-Descriptor-Execute Implementation
//
// This example demonstrates the v3 programming model applied to DeepSeek's
// Lightning Indexer TopK attention, showing how to handle:
// - Variable sequence lengths per batch
// - Discrete tier selection (2K/8K/64K/128K)
// - Complex nested loops with dynamic bounds
//
// Reference: docs/uv/dyn_llm_requirements.md Section 3 (Lightning Indexer)
// ============================================================================

#include <pto/pto-inst.hpp>
#include <pto/runtime/descriptor.hpp>
#include <pto/runtime/schedule.hpp>

// ============================================================================
// CONSTANTS
// ============================================================================

constexpr uint32_t BLOCK_SIZE = 128;
constexpr uint32_t INDEX_D = 128;
constexpr uint32_t SELECTED_COUNT = 2048;
constexpr uint32_t GROUP_SIZE = 8;

// ============================================================================
// SECTION 1: ITERATION SPACE DEFINITION
// ============================================================================

// The iteration space is 3D: (batch, seq_position, index_head)
// But seq_position has dynamic bounds AND affects the effective sequence length

PTO_SPACE(LightningIndexer) {
    // Batch dimension - static
    PTO_AXIS(batch, 0, batch_size)

    // Sequence position - typically S1 (query length)
    PTO_AXIS(seq_pos, 0, S1)

    // Index head dimension - n2 heads
    PTO_AXIS(idx_head, 0, n2)

    // No global tiling - each work item is one (batch, seq_pos, idx_head)
    // Internal tiling happens within kernel based on eff_seq
}

// ============================================================================
// SECTION 2: KERNEL TIERS
//
// The key insight from the requirements: effSeq determines the TopK path
// effSeq = curSeq - (S1 - s1Idx - 1)  -- changes per seq_pos!
//
// We pre-compile 4 tier kernels optimized for different eff_seq ranges
// ============================================================================

PTO_TIER_DEF(IndexerKernel) {
    // Each tier has different padding strategy for TopK
    PTO_TIER(eff_seq <= 2048,    indexer_kernel_2k,   ws_2k)
    PTO_TIER(eff_seq <= 8192,    indexer_kernel_8k,   ws_8k)
    PTO_TIER(eff_seq <= 65536,   indexer_kernel_64k,  ws_64k)
    PTO_DEFAULT(                 indexer_kernel_128k, ws_128k)
}

// ============================================================================
// SECTION 3: SCHEDULING STRATEGY
//
// From the pypto guide: task ordering significantly affects performance
// - Same batch should have locality (L2 cache reuse)
// - But we also want to start batches early to hide latency
// ============================================================================

PTO_SCHEDULE(IndexerSchedule) {
    // Keep same batch on same core group for cache locality
    PTO_AFFINITY(batch, PTO_CORE_GROUP)

    // Within batch, process seq positions sequentially (data dependencies)
    PTO_ORDER(batch, PTO_INTERLEAVED(2))   // Start 2 batches in parallel
    PTO_ORDER(seq_pos, PTO_SEQUENTIAL)      // Must be sequential for causality
    PTO_ORDER(idx_head, PTO_PARALLEL)       // Heads are independent

    // Prefetch K cache for next seq position
    PTO_PREFETCH(seq_pos, 1)
}

// ============================================================================
// SECTION 4: DESCRIPTOR STRUCTURE
//
// Lightweight descriptor capturing all info needed for kernel execution
// ============================================================================

struct IndexerDescriptor {
    // Which kernel tier to use
    uint32_t tier;

    // Iteration indices
    uint32_t batch_idx;
    uint32_t seq_pos;
    uint32_t idx_head;

    // Dynamic bounds derived from runtime values
    uint32_t cur_seq;           // actSeqKey[batch_idx]
    uint32_t eff_seq;           // cur_seq - (S1 - seq_pos - 1)
    uint32_t act_block;         // ceil(eff_seq / BLOCK_SIZE)

    // Pre-computed tier-specific values
    uint32_t pad_length;        // 2048, 8192, 65536, or 128K

    // Workspace offset for this work item
    uint32_t workspace_offset;
};

// ============================================================================
// SECTION 5: DESCRIPTOR GENERATOR
//
// This runs on AICPU control thread (Thread 0)
// Key: O(1) per descriptor generation, just index arithmetic
// ============================================================================

PTO_DESCRIPTOR_GEN(LightningIndexer) {
    // Get input metadata
    const uint32_t* act_seq_key = PTO_GET_INPUT_META(kv_cache, actual_seq_lens);
    const uint32_t batch_size = PTO_GET_PARAM(batch_size);
    const uint32_t S1 = PTO_GET_PARAM(S1);
    const uint32_t n2 = PTO_GET_PARAM(n2);

    // Apply schedule ordering
    PTO_APPLY_SCHEDULE(IndexerSchedule);

    // Generate descriptors following schedule
    // The schedule determines iteration order, not the nested loop structure
    PTO_SCHEDULED_FOR(batch_idx, seq_pos, idx_head) {
        // Read runtime value for this batch
        uint32_t cur_seq = act_seq_key[batch_idx];

        // Compute effective sequence length (causal attention)
        // This is the key dynamic value that determines which tier to use
        uint32_t causal_offset = S1 - seq_pos - 1;
        uint32_t eff_seq = cur_seq - causal_offset;

        // Select tier based on eff_seq
        uint32_t tier = PTO_SELECT_TIER(IndexerKernel, eff_seq);

        // Compute derived values
        uint32_t act_block = PTO_CEIL_DIV(eff_seq, BLOCK_SIZE);
        uint32_t pad_length = PTO_TIER_PARAM(tier, pad_length);

        // Emit descriptor
        PTO_EMIT_DESCRIPTOR(IndexerDescriptor{
            .tier = tier,
            .batch_idx = batch_idx,
            .seq_pos = seq_pos,
            .idx_head = idx_head,
            .cur_seq = cur_seq,
            .eff_seq = eff_seq,
            .act_block = act_block,
            .pad_length = pad_length,
            .workspace_offset = PTO_ALLOC_WORKSPACE(tier, batch_idx, seq_pos, idx_head),
        });
    }
}

// ============================================================================
// SECTION 6: KERNEL IMPLEMENTATIONS
//
// Each tier is optimized for its eff_seq range
// The kernel reads bounds from descriptor, not computed at runtime
// ============================================================================

// 8K tier implementation (medium sequences)
PTO_KERNEL_IMPL(indexer_kernel_8k) {
    auto desc = PTO_GET_DESCRIPTOR<IndexerDescriptor>();

    // Load query for this position
    // Q shape: [batch, S1, indexN1, indexD]
    VecTile<half, GROUP_SIZE, INDEX_D> q_tile;
    TLOAD(q_tile, query, {desc.batch_idx, desc.seq_pos, desc.idx_head * GROUP_SIZE});

    // Load weights for this position
    VecTile<half, GROUP_SIZE, 1> weight_tile;
    TLOAD(weight_tile, weights, {desc.batch_idx, desc.seq_pos, desc.idx_head * GROUP_SIZE});

    // Allocate local sum buffer (eff_seq elements)
    // Workspace was pre-allocated based on tier
    __ubuf__ float* local_sum = PTO_GET_WORKSPACE<float>(desc.workspace_offset);

    // Initialize local sum to zero
    VecTile<float, 1, desc.eff_seq> sum_tile;
    TASSIGN(sum_tile, 0.0f);

    // Process each block in the effective sequence
    // act_block = ceil(eff_seq / BLOCK_SIZE) -- from descriptor
    for (uint32_t block_idx = 0; block_idx < desc.act_block; ++block_idx) {
        // Get physical block index from block table
        uint32_t phys_block = PTO_LOAD_SCALAR(block_table,
            {desc.batch_idx, block_idx});

        // Load K tile from paged KV cache
        // K shape: [block_num, BLOCK_SIZE, n2, indexD]
        RightTile<half, BLOCK_SIZE, INDEX_D> k_tile;
        TLOAD(k_tile, key_cache, {phys_block, 0, desc.idx_head});

        // Compute Q * K^T
        // Result: [GROUP_SIZE, BLOCK_SIZE]
        MatTile<float, GROUP_SIZE, BLOCK_SIZE> mm_result;
        TMATMUL(mm_result, q_tile, k_tile);

        // Apply ReLU
        TRELU(mm_result, mm_result);

        // Multiply by weights and sum along group dimension
        // weight_tile: [GROUP_SIZE, 1]
        // mm_result: [GROUP_SIZE, BLOCK_SIZE]
        MatTile<float, GROUP_SIZE, BLOCK_SIZE> weighted;
        TMUL_BROADCAST(weighted, mm_result, weight_tile);

        // Sum along group dimension
        VecTile<float, 1, BLOCK_SIZE> block_sum;
        TCOLSUM(block_sum, weighted);

        // Accumulate to local sum at correct offset
        uint32_t start_pos = block_idx * BLOCK_SIZE;
        uint32_t valid_len = PTO_MIN(BLOCK_SIZE, desc.eff_seq - start_pos);

        // Write to local sum buffer
        TSTORE_PARTIAL(local_sum, block_sum, start_pos, valid_len);
    }

    // TopK selection on local_sum
    // Pad to tier-specific length (8192 for 8K tier)
    VecTile<float, 1, 8192> padded_sum;
    TASSIGN(padded_sum, -FLT_MAX);  // Pad with -inf
    TCOPY_PARTIAL(padded_sum, local_sum, 0, desc.eff_seq);

    // TopK operation
    VecTile<int32_t, 1, SELECTED_COUNT> topk_indices;
    VecTile<float, 1, SELECTED_COUNT> topk_values;
    TTOPK(topk_values, topk_indices, padded_sum, SELECTED_COUNT);

    // Mask invalid indices (those pointing to padding)
    for (uint32_t i = 0; i < SELECTED_COUNT; ++i) {
        if (topk_indices[i] >= desc.eff_seq) {
            topk_indices[i] = -1;  // Mark as invalid
        }
    }

    // Store result
    // topk_result: [batch, S1, n2, SELECTED_COUNT]
    TSTORE(topk_result, topk_indices,
        {desc.batch_idx, desc.seq_pos, desc.idx_head});
}

// 2K tier - optimized for short sequences
PTO_KERNEL_IMPL(indexer_kernel_2k) {
    auto desc = PTO_GET_DESCRIPTOR<IndexerDescriptor>();

    // Similar structure but with 2K padding
    // Can use more aggressive register tiling for short sequences

    // ... (similar implementation with 2K-specific optimizations)
}

// 64K tier - optimized for long sequences
PTO_KERNEL_IMPL(indexer_kernel_64k) {
    auto desc = PTO_GET_DESCRIPTOR<IndexerDescriptor>();

    // Needs chunked processing to fit in L1/UB
    // Multiple TopK passes with merging

    // ... (implementation with chunked TopK)
}

// 128K tier - maximum sequence support
PTO_KERNEL_IMPL(indexer_kernel_128k) {
    auto desc = PTO_GET_DESCRIPTOR<IndexerDescriptor>();

    // Most complex tier with hierarchical TopK

    // ... (implementation with hierarchical TopK)
}

// ============================================================================
// SECTION 7: HOST ENTRY POINT
// ============================================================================

void lightning_indexer_launch(
    Tensor query,           // [batch, S1, indexN1, indexD]
    Tensor weights,         // [batch, S1, indexN1]
    PagedKVCache key_cache, // [block_num, BLOCK_SIZE, n2, indexD]
    Tensor block_table,     // [batch, max_blocks]
    uint32_t* act_seq_key,  // [batch] actual sequence lengths
    Tensor topk_result      // [batch, S1, n2, SELECTED_COUNT]
) {
    PTO_LAUNCH(LightningIndexer, {
        .query = query,
        .weights = weights,
        .key_cache = key_cache,
        .block_table = block_table,
        .actual_seq_lens = act_seq_key,
        .topk_result = topk_result,
    });
}

// ============================================================================
// SECTION 8: COMPARISON WITH ORIGINAL REQUIREMENTS
//
// Original (from dyn_llm_requirements.md):
//   for bIdx in range(B):
//       curSeq = actSeqKey[bIdx]
//       for s1Idx in range(S1):
//           effSeq = curSeq - (S1 - s1Idx - 1)
//           actBlock = ceil(effSeq / blockSize)
//           for n2Idx in range(n2):
//               # Dynamic loop over actBlock
//               # Dynamic TopK path selection
//
// v3 Model:
// - Iteration space captures (batch, seq_pos, idx_head)
// - Descriptor captures (eff_seq, act_block, tier) per work item
// - Tier selection handles 2K/8K/64K/128K paths
// - Kernel receives all bounds from descriptor
// - No runtime shape decisions in kernel
//
// Benefits:
// 1. Descriptor generation is O(1) per work item
// 2. Tier-specific optimizations for each sequence range
// 3. Schedule controls execution order for locality
// 4. Same kernel code handles all dynamic patterns
// ============================================================================
