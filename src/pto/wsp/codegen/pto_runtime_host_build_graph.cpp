// Copyright 2026 PTO-WSP Authors
// SPDX-License-Identifier: MIT

#include "pto/wsp/codegen/pto_runtime_host_build_graph.hpp"

#include <functional>
#include <stdexcept>
#include <sstream>
#include <unordered_map>
#include <variant>
#include <vector>

namespace pto::wsp::codegen::pto_runtime {

static std::string emit_kernel_config_py(const pto::wsp::ir::Module& module) {
    std::ostringstream out;
    out << "\"\"\"\n"
           "Kernel and orchestration configuration (PTO-WSP generated)\n"
           "\n"
           "This file mirrors pto-runtime's example `kernel_config.py` conventions.\n"
           "\"\"\"\n\n";
    out << "import os\n";
    out << "from pathlib import Path\n\n";
    out << "_KERNELS_ROOT = Path(__file__).parent\n\n";
    out << "ORCHESTRATION = {\n";
    out << "    \"source\": str(_KERNELS_ROOT / \"orchestration\" / \"pto_wsp_orch.cpp\"),\n";
    out << "    \"function_name\": \"build_pto_wsp_graph\",\n";
    out << "}\n\n";

    out << "KERNELS_A2A3SIM = [\n";
    for (size_t i = 0; i < module.kernels.size(); ++i) {
        const auto& k = module.kernels[i];
        out << "    {\"func_id\": " << i << ", \"source\": str(_KERNELS_ROOT / \"aiv_sim\" / \"kernel_" << k.name
            << ".cpp\"), \"core_type\": \"aiv\"},\n";
    }
    out << "]\n\n";

    out << "KERNELS_A2A3 = [\n";
    for (size_t i = 0; i < module.kernels.size(); ++i) {
        const auto& k = module.kernels[i];
        out << "    {\"func_id\": " << i << ", \"source\": str(_KERNELS_ROOT / \"aiv\" / \"kernel_" << k.name
            << ".cpp\"), \"core_type\": \"aiv\"},\n";
    }
    out << "]\n\n";

    out << "PTO_RUNTIME_PLATFORM = os.environ.get(\"PTO_RUNTIME_PLATFORM\", \"\")\n";
    out << "KERNELS = KERNELS_A2A3SIM if PTO_RUNTIME_PLATFORM == \"a2a3sim\" else KERNELS_A2A3\n";
    return out.str();
}

static std::string emit_orchestration_cpp(const pto::wsp::ir::Module& module) {
    std::ostringstream out;
    out << "// Generated by PTO-WSP v10 (Phase 1)\n";
    out << "// Target: pto-runtime host_build_graph\n";
    out << "//\n";
    out << "// NOTE: This orchestration is intentionally Phase-1-scoped:\n";
    out << "// - host builds a static dependency graph\n";
    out << "// - no CSP/channel semantics yet\n";
    out << "// - no multi-AICPU scheduler assignment yet (see TODO marker below)\n";
    out << "\n";
    out << "#include \"runtime.h\"\n";
    out << "#include <iostream>\n";
    out << "#include <cstdint>\n";
    out << "#include <cstddef>\n";
    out << "\n";
    out << "namespace {\n";
    out << "static int pto_wsp_select_aicpu_scheduler_id(uint64_t task_id) {\n";
    out << "  (void)task_id;\n";
    out << "  // TODO_PTO_RUNTIME_MULTI_AICPU_DISPATCH: map PTO-WSP dispatch(policy) to AICPU scheduler assignment.\n";
    out << "  // pto-runtime host_build_graph does not yet expose a stable API for taskâ†’AICPU-shard selection.\n";
    out << "  return 0;\n";
    out << "}\n";
    out << "}  // namespace\n\n";

    if (module.workloads.empty() || !module.workloads[0].body) {
        out << "extern \"C\" {\n";
        out << "int build_pto_wsp_graph(Runtime*, uint64_t*, int) { return -1; }\n";
        out << "}  // extern \"C\"\n";
        return out.str();
    }

    const auto& wl = module.workloads[0];

    // Kernel name -> func_id (index in module.kernels)
    std::unordered_map<std::string, int> func_id_by_name;
    func_id_by_name.reserve(module.kernels.size());
    for (size_t i = 0; i < module.kernels.size(); ++i) {
        func_id_by_name.emplace(module.kernels[i].name, static_cast<int>(i));
    }

    auto dtype_size_bytes = [](pto::wsp::ir::DType t) -> int64_t {
        switch (t) {
            case pto::wsp::ir::DType::F16: return 2;
            case pto::wsp::ir::DType::BF16: return 2;
            case pto::wsp::ir::DType::F32: return 4;
            case pto::wsp::ir::DType::F64: return 8;
            case pto::wsp::ir::DType::I8: return 1;
            case pto::wsp::ir::DType::I16: return 2;
            case pto::wsp::ir::DType::I32: return 4;
            case pto::wsp::ir::DType::I64: return 8;
            case pto::wsp::ir::DType::U8: return 1;
            case pto::wsp::ir::DType::U16: return 2;
            case pto::wsp::ir::DType::U32: return 4;
            case pto::wsp::ir::DType::U64: return 8;
            case pto::wsp::ir::DType::Bool: return 1;
        }
        return 4;
    };

    auto shape_to_row_major_strides = [](const std::vector<int64_t>& shape) -> std::vector<int64_t> {
        std::vector<int64_t> stride(shape.size(), 1);
        int64_t acc = 1;
        for (int i = static_cast<int>(shape.size()) - 1; i >= 0; --i) {
            stride[static_cast<size_t>(i)] = acc;
            const int64_t d = shape[static_cast<size_t>(i)];
            if (d <= 0) throw std::runtime_error("pto-runtime Phase 1: dynamic/unknown tensor shapes not supported");
            acc *= d;
        }
        return stride;
    };

    auto kernel_param_is_tensor = [&](const pto::wsp::ir::CodegenKernelDef& kd, const std::string& param_name) -> bool {
        for (const auto& p : kd.params) {
            if (p.name != param_name) continue;
            auto it = kd.values.find(p.id);
            if (it == kd.values.end()) return false;
            return it->second.has_shape;
        }
        return false;
    };

    auto kernel_tile_elems = [&](const pto::wsp::ir::CodegenKernelDef& kd) -> int64_t {
        // Phase 1: assume all tensor params share the same tile shape; take the first tensor param.
        for (const auto& p : kd.params) {
            auto it = kd.values.find(p.id);
            if (it == kd.values.end()) continue;
            if (!it->second.has_shape) continue;
            const int64_t rows = static_cast<int64_t>(it->second.rows);
            const int64_t cols = static_cast<int64_t>(it->second.cols);
            if (rows <= 0 || cols <= 0) throw std::runtime_error("pto-runtime Phase 1: invalid tile shape");
            return rows * cols;
        }
        throw std::runtime_error("pto-runtime Phase 1: kernel has no tensor params");
    };

    auto index_expr_to_cpp = [&](const pto::wsp::ir::CodegenIndexExpr& e) -> std::string {
        if (e.is_var) return "(int64_t)" + e.var;
        return "(int64_t)" + std::to_string(e.constant);
    };

    // Emit helpers for tensor strides / elem sizes.
    out << "static const int PTO_WSP_NUM_TENSORS = " << module.tensors.size() << ";\n";
    out << "static const int64_t PTO_WSP_TENSOR_ELEM_SIZE[PTO_WSP_NUM_TENSORS] = {";
    for (size_t i = 0; i < module.tensors.size(); ++i) {
        if (i) out << ", ";
        out << dtype_size_bytes(module.tensors[i].dtype);
    }
    out << "};\n";
    for (size_t i = 0; i < module.tensors.size(); ++i) {
        const auto strides = shape_to_row_major_strides(module.tensors[i].shape);
        out << "static const int64_t PTO_WSP_T" << i << "_STRIDE[" << strides.size() << "] = {";
        for (size_t d = 0; d < strides.size(); ++d) {
            if (d) out << ", ";
            out << strides[d];
        }
        out << "};\n";
    }
    out << "\n";

    struct EmitCtx {
        int next_task_tmp = 0;
        std::string prev_task_var = "prev_task";
    };
    EmitCtx ctx;

    auto emit_line = [&](int indent, const std::string& s) {
        out << std::string(static_cast<size_t>(indent) * 2, ' ') << s << "\n";
    };

    std::function<void(const pto::wsp::ir::IRPtr<pto::wsp::ir::WorkloadNode>&)> emit_node;
    emit_node = [&](const auto& n) {
        using pto::wsp::ir::NodeKind;
        if (!n) return;

        switch (n->kind) {
            case NodeKind::Task: {
                const auto& t = static_cast<const pto::wsp::ir::TaskNode&>(*n);
                auto itf = func_id_by_name.find(t.kernel_name);
                if (itf == func_id_by_name.end()) {
                    throw std::runtime_error("pto-runtime Phase 1: unknown kernel name: " + t.kernel_name);
                }
                const int func_id = itf->second;
                const auto& kd = module.kernels[static_cast<size_t>(func_id)];

                // Build map param -> tensor_id.
                std::unordered_map<std::string, uint32_t> tensor_id_by_param;
                tensor_id_by_param.reserve(t.tensor_args.size());
                for (const auto& ta : t.tensor_args) {
                    tensor_id_by_param.emplace(ta.param, ta.tensor_id);
                }

                // Phase 1 task ABI (per-kernel invocation):
                //   [tensor_ptrs..., scalar_u64..., size_elems]
                int num_tensor_params = 0;
                int num_scalar_params = 0;
                for (const auto& p : kd.params) {
                    if (kernel_param_is_tensor(kd, p.name)) num_tensor_params++;
                    else num_scalar_params++;
                }
                const int num_args = num_tensor_params + num_scalar_params + 1;
                const int64_t size_elems = kernel_tile_elems(kd);

                const std::string tmp = "t" + std::to_string(ctx.next_task_tmp++);
                emit_line(2, "{");
                emit_line(3, "uint64_t args_" + tmp + "[" + std::to_string(num_args) + "];");

                int tensor_idx = 0;
                int scalar_idx = 0;
                for (const auto& p : kd.params) {
                    if (kernel_param_is_tensor(kd, p.name)) {
                        auto itt = tensor_id_by_param.find(p.name);
                        if (itt == tensor_id_by_param.end()) {
                            throw std::runtime_error("pto-runtime Phase 1: missing tensor binding for param: " + p.name);
                        }
                        const uint32_t tid = itt->second;

                        // Compute view base pointer (best-effort) from CodegenTensorArg.index_exprs.
                        const pto::wsp::ir::CodegenTensorArg* ta = nullptr;
                        for (const auto& x : t.tensor_args) {
                            if (x.param == p.name) { ta = &x; break; }
                        }
                        if (!ta) throw std::runtime_error("pto-runtime Phase 1: missing tensor arg for param: " + p.name);

                        emit_line(3, "{");
                        emit_line(4, "int64_t off_elems = 0;");
                        for (size_t d = 0; d < ta->index_exprs.size(); ++d) {
                            emit_line(
                                4,
                                "off_elems += (" + index_expr_to_cpp(ta->index_exprs[d]) + ") * (int64_t)PTO_WSP_T" +
                                    std::to_string(tid) + "_STRIDE[" + std::to_string(d) + "];");
                        }
                        emit_line(
                            4,
                            "uint8_t* ptr = (uint8_t*)dev_ptrs[" + std::to_string(tid) + "] + off_elems * PTO_WSP_TENSOR_ELEM_SIZE[" +
                                std::to_string(tid) + "];");
                        emit_line(4, "args_" + tmp + "[" + std::to_string(tensor_idx) + "] = (uint64_t)(uintptr_t)ptr;");
                        emit_line(3, "}");
                        tensor_idx++;
                    } else {
                        if (static_cast<size_t>(scalar_idx) >= t.scalar_args.size()) {
                            throw std::runtime_error("pto-runtime Phase 1: missing scalar binding for kernel: " + kd.name);
                        }
                        const uint64_t v = t.scalar_args[static_cast<size_t>(scalar_idx)].u64;
                        std::ostringstream hex;
                        hex << "0x" << std::hex << v << "ULL";
                        emit_line(
                            3,
                            "args_" + tmp + "[" + std::to_string(num_tensor_params + scalar_idx) + "] = (uint64_t)" + hex.str() +
                                ";");
                        scalar_idx++;
                    }
                }
                emit_line(
                    3,
                    "args_" + tmp + "[" + std::to_string(num_args - 1) + "] = (uint64_t)" + std::to_string(size_elems) +
                        "ULL;");
                emit_line(
                    3,
                    "int " + tmp + " = runtime->add_task(args_" + tmp + ", " + std::to_string(num_args) + ", " +
                        std::to_string(func_id) + ", /*core_type=*/1);");
                emit_line(3, "if (" + ctx.prev_task_var + " >= 0) runtime->add_successor(" + ctx.prev_task_var + ", " + tmp + ");");
                emit_line(3, ctx.prev_task_var + " = " + tmp + ";");
                emit_line(2, "}");
                return;
            }

            case NodeKind::Sequential: {
                const auto& s = static_cast<const pto::wsp::ir::SequentialNode&>(*n);
                for (const auto& w : s.workloads) emit_node(w);
                return;
            }

            case NodeKind::Combine: {
                const auto& c = static_cast<const pto::wsp::ir::CombineNode&>(*n);
                // Phase 1 conservative: sequentially chain combine children.
                for (const auto& w : c.workloads) emit_node(w);
                return;
            }

            case NodeKind::ParallelFor: {
                const auto& pf = static_cast<const pto::wsp::ir::ParallelForNode&>(*n);
                if (!pf.axis) throw std::runtime_error("pto-runtime Phase 1: parallel_for axis is null");
                if (pf.axis->kind != NodeKind::DenseAxis) {
                    throw std::runtime_error("pto-runtime Phase 1: only dense[N] parallel_for is supported");
                }
                const auto& ax = static_cast<const pto::wsp::ir::DenseAxisNode&>(*pf.axis);
                emit_line(2, "for (int64_t " + pf.index_var + " = 0; " + pf.index_var + " < " +
                                   std::to_string(ax.size) + "; ++" + pf.index_var + ") {");
                emit_node(pf.body);
                emit_line(2, "}");
                return;
            }

            default:
                throw std::runtime_error("pto-runtime Phase 1: unsupported workload node kind");
        }
    };

    out << "extern \"C\" {\n\n";
    out << "int build_pto_wsp_graph(Runtime* runtime, uint64_t* args, int arg_count) {\n";
    out << "  if (!runtime || !args) return -1;\n";
    out << "  const int expected_args = PTO_WSP_NUM_TENSORS * 2;\n";
    out << "  if (arg_count < expected_args) {\n";
    out << "    std::cerr << \"build_pto_wsp_graph: expected at least \" << expected_args << \" args, got \" << arg_count << '\\n';\n";
    out << "    return -1;\n";
    out << "  }\n\n";

    out << "  // Phase 1 ABI: args = [ptr0, nbytes0, ptr1, nbytes1, ...] for each tensor.\n";
    out << "  void* host_ptrs[PTO_WSP_NUM_TENSORS];\n";
    out << "  size_t nbytes[PTO_WSP_NUM_TENSORS];\n";
    out << "  for (int i = 0; i < PTO_WSP_NUM_TENSORS; ++i) {\n";
    out << "    host_ptrs[i] = (void*)(uintptr_t)args[2 * i];\n";
    out << "    nbytes[i] = (size_t)args[2 * i + 1];\n";
    out << "  }\n\n";

    out << "  void* dev_ptrs[PTO_WSP_NUM_TENSORS];\n";
    out << "  for (int i = 0; i < PTO_WSP_NUM_TENSORS; ++i) dev_ptrs[i] = nullptr;\n\n";

    out << "  // Allocate device buffers and copy inputs. Record all tensors for copy-back (safe baseline).\n";
    out << "  for (int i = 0; i < PTO_WSP_NUM_TENSORS; ++i) {\n";
    out << "    dev_ptrs[i] = runtime->host_api.device_malloc(nbytes[i]);\n";
    out << "    if (!dev_ptrs[i]) return -1;\n";
    out << "    if (runtime->host_api.copy_to_device(dev_ptrs[i], host_ptrs[i], nbytes[i]) != 0) return -1;\n";
    out << "    runtime->record_tensor_pair(host_ptrs[i], dev_ptrs[i], nbytes[i]);\n";
    out << "  }\n\n";

    out << "  // Silence unused placeholder hook.\n";
    out << "  (void)pto_wsp_select_aicpu_scheduler_id(0);\n\n";

    out << "  // Build tasks and dependencies (Phase 1: conservative sequential chaining).\n";
    out << "  int prev_task = -1;\n";
    emit_node(wl.body);

    out << "  return 0;\n";
    out << "}\n\n";
    out << "}  // extern \"C\"\n";
    return out.str();
}

static std::string emit_stub_aiv_kernel_cpp(const std::string& name) {
    std::ostringstream out;
    out << "// Generated by PTO-WSP v10 (Phase 1)\n";
    out << "// Target: pto-runtime AIV kernel (a2a3)\n\n";
    out << "#include <cstdint>\n";
    out << "#include <pto/pto-inst.hpp>\n";
    out << "#include <pto/common/constants.hpp>\n";
    out << "#include <cmath>\n";
    out << "#include <algorithm>\n\n";
    out << "using namespace pto;\n\n";
    out << "#ifndef __gm__\n";
    out << "#define __gm__\n";
    out << "#endif\n\n";
    out << "#ifndef __aicore__\n";
    out << "#define __aicore__ [aicore]\n";
    out << "#endif\n\n";
    out << "extern \"C\" __aicore__ __attribute__((always_inline)) void " << name << "(__gm__ int64_t* args) {\n";
    out << "  (void)args;\n";
    out << "  // NOTE: Phase 1 uses a conservative, loop-based lowering for correctness.\n";
    out << "}\n";
    return out.str();
}

static std::string emit_elementwise_kernel_body_f32(
    const pto::wsp::ir::CodegenKernelDef& kd,
    const std::string& dst_param_name,
    int num_tensor_params,
    int num_scalar_params) {
    // Phase 1 kernel ABI:
    //   args[0..T-1] = tensor pointers (float*)
    //   args[T..T+S-1] = scalar u64 bit-patterns (f32)
    //   args[T+S] = size_elems
    std::unordered_map<int, const pto::wsp::ir::CodegenKernelOpInfo*> op_by_result;
    for (const auto& op : kd.ops) {
        if (op.has_result) op_by_result.emplace(op.result, &op);
    }

    // Identify store: dst (tensor param) <- src (value)
    const pto::wsp::ir::CodegenKernelOpInfo* store = nullptr;
    for (auto it = kd.ops.rbegin(); it != kd.ops.rend(); ++it) {
        if (it->kind == "Store") { store = &*it; break; }
    }
    if (!store) throw std::runtime_error("pto-runtime Phase 1: kernel has no Store op: " + kd.name);
    if (store->operands.size() != 2) throw std::runtime_error("pto-runtime Phase 1: Store expects 2 operands");

    const int dst_id = store->operands[0];
    const int src_id = store->operands[1];

    // Offset must be [0,0] for Phase 1.
    auto it_off = store->attrs.find("offset");
    if (it_off != store->attrs.end()) {
        if (!std::holds_alternative<std::vector<int64_t>>(it_off->second)) {
            throw std::runtime_error("pto-runtime Phase 1: Store offset attr has unexpected type");
        }
        const auto& off = std::get<std::vector<int64_t>>(it_off->second);
        if (!(off.size() == 2 && off[0] == 0 && off[1] == 0)) {
            throw std::runtime_error("pto-runtime Phase 1: non-zero Store offset not supported");
        }
    }

    // Map params to arg indices.
    std::unordered_map<int, std::string> tensor_ptr_by_value_id;
    std::unordered_map<int, std::string> scalar_val_by_value_id;
    int tensor_idx = 0;
    int scalar_idx = 0;
    for (const auto& p : kd.params) {
        auto itv = kd.values.find(p.id);
        if (itv == kd.values.end()) throw std::runtime_error("pto-runtime Phase 1: kernel param missing value");
        if (itv->second.dtype != pto::wsp::ir::DType::F32) {
            throw std::runtime_error("pto-runtime Phase 1: only f32 kernels supported (for now)");
        }
        if (itv->second.has_shape) {
            tensor_ptr_by_value_id.emplace(p.id, p.name);
            tensor_idx++;
        } else {
            scalar_val_by_value_id.emplace(p.id, p.name);
            scalar_idx++;
        }
    }
    if (tensor_idx != num_tensor_params || scalar_idx != num_scalar_params) {
        throw std::runtime_error("pto-runtime Phase 1: internal arg counting mismatch");
    }

    std::function<std::string(int)> emit_value;
    emit_value = [&](int vid) -> std::string {
        auto it_t = tensor_ptr_by_value_id.find(vid);
        if (it_t != tensor_ptr_by_value_id.end()) return it_t->second + "[i]";
        auto it_s = scalar_val_by_value_id.find(vid);
        if (it_s != scalar_val_by_value_id.end()) return it_s->second;

        auto it_op = op_by_result.find(vid);
        if (it_op == op_by_result.end()) {
            throw std::runtime_error("pto-runtime Phase 1: unknown value id in kernel: " + kd.name);
        }
        const auto& op = *it_op->second;
        if (op.kind == "Mul" || op.kind == "Add" || op.kind == "Sub" || op.kind == "Div" ||
            op.kind == "Max" || op.kind == "Min") {
            if (op.operands.size() != 2) throw std::runtime_error("pto-runtime Phase 1: binary op expects 2 operands");
            const std::string a = emit_value(op.operands[0]);
            const std::string b = emit_value(op.operands[1]);
            if (op.kind == "Mul") return "(" + a + " * " + b + ")";
            if (op.kind == "Add") return "(" + a + " + " + b + ")";
            if (op.kind == "Sub") return "(" + a + " - " + b + ")";
            if (op.kind == "Div") return "(" + a + " / " + b + ")";
            if (op.kind == "Max") return "std::max(" + a + ", " + b + ")";
            return "std::min(" + a + ", " + b + ")";
        }
        if (op.kind == "Exp" || op.kind == "Rsqrt") {
            if (op.operands.size() != 1) throw std::runtime_error("pto-runtime Phase 1: unary op expects 1 operand");
            const std::string a = emit_value(op.operands[0]);
            if (op.kind == "Exp") return "std::exp(" + a + ")";
            return "(1.0f / std::sqrt(" + a + "))";
        }

        throw std::runtime_error("pto-runtime Phase 1: unsupported kernel op: " + op.kind);
    };

    // dst_id must map to a tensor param.
    if (tensor_ptr_by_value_id.find(dst_id) == tensor_ptr_by_value_id.end()) {
        throw std::runtime_error("pto-runtime Phase 1: Store dst is not a tensor param");
    }
    (void)dst_param_name;

    std::ostringstream body;
    body << "  int size = (int)args[" << (num_tensor_params + num_scalar_params) << "];\n";
    body << "  for (int i = 0; i < size; ++i) {\n";
    body << "    " << dst_param_name << "[i] = " << emit_value(src_id) << ";\n";
    body << "  }\n";
    return body.str();
}

static std::string emit_aiv_sim_kernel_cpp(const pto::wsp::ir::CodegenKernelDef& kd) {
    const std::string fn = "kernel_" + kd.name;

    // Count tensor/scalar params and identify output param (store dst).
    int num_tensor_params = 0;
    int num_scalar_params = 0;
    for (const auto& p : kd.params) {
        auto itv = kd.values.find(p.id);
        if (itv == kd.values.end()) throw std::runtime_error("pto-runtime Phase 1: kernel param missing value");
        if (itv->second.has_shape) num_tensor_params++;
        else num_scalar_params++;
    }

    const pto::wsp::ir::CodegenKernelOpInfo* store = nullptr;
    for (auto it = kd.ops.rbegin(); it != kd.ops.rend(); ++it) {
        if (it->kind == "Store") { store = &*it; break; }
    }
    if (!store || store->operands.size() != 2) throw std::runtime_error("pto-runtime Phase 1: kernel must end with Store");
    const int dst_id = store->operands[0];

    std::string dst_param;
    for (const auto& p : kd.params) {
        if (p.id == dst_id) dst_param = p.name;
    }
    if (dst_param.empty()) throw std::runtime_error("pto-runtime Phase 1: Store dst must be a kernel param");

    std::ostringstream out;
    out << "// Generated by PTO-WSP v10 (Phase 1)\n";
    out << "// Target: pto-runtime a2a3sim kernel (host execution)\n\n";
    out << "#include <cstdint>\n";
    out << "#include <cmath>\n";
    out << "#include <algorithm>\n\n";
    out << "extern \"C\" void " << fn << "(int64_t* args) {\n";

    int tensor_idx = 0;
    int scalar_idx = 0;
    for (const auto& p : kd.params) {
        auto itv = kd.values.find(p.id);
        if (itv == kd.values.end()) throw std::runtime_error("pto-runtime Phase 1: kernel param missing value");
        if (itv->second.dtype != pto::wsp::ir::DType::F32) {
            throw std::runtime_error("pto-runtime Phase 1: only f32 kernels supported (for now)");
        }
        if (itv->second.has_shape) {
            out << "  float* " << p.name << " = reinterpret_cast<float*>((uintptr_t)args[" << tensor_idx << "]);\n";
            tensor_idx++;
        } else {
            out << "  union { uint64_t u64; float f32; } _" << p.name << ";\n";
            out << "  _" << p.name << ".u64 = (uint64_t)args[" << (num_tensor_params + scalar_idx) << "];\n";
            out << "  float " << p.name << " = _" << p.name << ".f32;\n";
            scalar_idx++;
        }
    }

    out << emit_elementwise_kernel_body_f32(kd, dst_param, num_tensor_params, num_scalar_params);
    out << "}\n";
    return out.str();
}

static std::string emit_aiv_kernel_cpp(const pto::wsp::ir::CodegenKernelDef& kd) {
    const std::string fn = "kernel_" + kd.name;

    int num_tensor_params = 0;
    int num_scalar_params = 0;
    for (const auto& p : kd.params) {
        auto itv = kd.values.find(p.id);
        if (itv == kd.values.end()) throw std::runtime_error("pto-runtime Phase 1: kernel param missing value");
        if (itv->second.has_shape) num_tensor_params++;
        else num_scalar_params++;
    }

    const pto::wsp::ir::CodegenKernelOpInfo* store = nullptr;
    for (auto it = kd.ops.rbegin(); it != kd.ops.rend(); ++it) {
        if (it->kind == "Store") { store = &*it; break; }
    }
    if (!store || store->operands.size() != 2) throw std::runtime_error("pto-runtime Phase 1: kernel must end with Store");
    const int dst_id = store->operands[0];

    std::string dst_param;
    for (const auto& p : kd.params) {
        if (p.id == dst_id) dst_param = p.name;
    }
    if (dst_param.empty()) throw std::runtime_error("pto-runtime Phase 1: Store dst must be a kernel param");

    std::ostringstream out;
    out << "// Generated by PTO-WSP v10 (Phase 1)\n";
    out << "// Target: pto-runtime AIV kernel (a2a3)\n\n";
    out << "#include <cstdint>\n";
    out << "#include <pto/pto-inst.hpp>\n";
    out << "#include <pto/common/constants.hpp>\n";
    out << "#include <cmath>\n";
    out << "#include <algorithm>\n\n";
    out << "using namespace pto;\n\n";
    out << "#ifndef __gm__\n";
    out << "#define __gm__\n";
    out << "#endif\n\n";
    out << "#ifndef __aicore__\n";
    out << "#define __aicore__ [aicore]\n";
    out << "#endif\n\n";
    out << "extern \"C\" __aicore__ __attribute__((always_inline)) void " << fn << "(__gm__ int64_t* args) {\n";

    int tensor_idx = 0;
    int scalar_idx = 0;
    for (const auto& p : kd.params) {
        auto itv = kd.values.find(p.id);
        if (itv == kd.values.end()) throw std::runtime_error("pto-runtime Phase 1: kernel param missing value");
        if (itv->second.dtype != pto::wsp::ir::DType::F32) {
            throw std::runtime_error("pto-runtime Phase 1: only f32 kernels supported (for now)");
        }
        if (itv->second.has_shape) {
            out << "  __gm__ float* " << p.name << " = reinterpret_cast<__gm__ float*>((uint64_t)args[" << tensor_idx
                << "]);\n";
            tensor_idx++;
        } else {
            out << "  union { uint64_t u64; float f32; } _" << p.name << ";\n";
            out << "  _" << p.name << ".u64 = (uint64_t)args[" << (num_tensor_params + scalar_idx) << "];\n";
            out << "  float " << p.name << " = _" << p.name << ".f32;\n";
            scalar_idx++;
        }
    }

    out << emit_elementwise_kernel_body_f32(kd, dst_param, num_tensor_params, num_scalar_params);
    out << "}\n";
    return out.str();
}

std::map<std::string, std::string> emit_host_build_graph_sources(
    const ir::Module& module,
    const backend::CompileOptions& /*options*/) {
    std::map<std::string, std::string> sources;
    sources.emplace("kernels/kernel_config.py", emit_kernel_config_py(module));
    sources.emplace("kernels/orchestration/pto_wsp_orch.cpp", emit_orchestration_cpp(module));
    for (const auto& kd : module.kernels) {
        sources.emplace("kernels/aiv/kernel_" + kd.name + ".cpp", emit_aiv_kernel_cpp(kd));
        sources.emplace("kernels/aiv_sim/kernel_" + kd.name + ".cpp", emit_aiv_sim_kernel_cpp(kd));
    }
    return sources;
}

}  // namespace pto::wsp::codegen::pto_runtime
