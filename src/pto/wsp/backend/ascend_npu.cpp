// PTO Workload-Schedule Programming (PTO-WSP) framework v9 - Ascend NPU Backend Implementation
// Copyright (c) 2026 PTO Project
// SPDX-License-Identifier: MIT

#include "pto/rt/backend/ascend_npu.hpp"

namespace pto::wsp::backend::ascend {

// ============================================================
// AscendEmitter Implementation
// ============================================================

std::string AscendEmitter::name() const { return "ascend_npu"; }

std::string AscendEmitter::file_extension() const { return ".cpp"; }

void AscendEmitter::emit_header(codegen::CodeGenContext& ctx) {
    ctx.emit("// Generated by PTO-RT v9 Ascend Backend");
    ctx.emit("// Target: Ascend NPU (CANN Kernel)");
    ctx.emit("");
    ctx.emit("#include <ascend_kernel.h>");
    ctx.emit("#include <pto_tile.hpp>");
    ctx.emit("");
    ctx.emit("using namespace pto;");
    ctx.emit("");
}

void AscendEmitter::emit_footer(codegen::CodeGenContext& ctx) {
    ctx.emit("// End of generated code");
}

void AscendEmitter::emit_npu_function(codegen::CodeGenContext& ctx, const ir::NPUFunction& func) {
    ctx.emit_comment("NPU Kernel: " + func.name);

    // Emit kernel signature
    std::ostringstream sig;
    sig << "__aicore__ void " << func.name << "(";

    // Add memref parameters
    bool first = true;
    for (const auto& m : func.memrefs) {
        if (!first) sig << ", ";
        first = false;
        sig << "GM_ADDR " << m.name;
    }
    sig << ")";
    ctx.emit(sig.str());
    ctx.emit("{");
    ctx.push_indent();

    // Emit tile declarations
    for (const auto& t : func.tiles) {
        emit_tile_decl(ctx, t);
    }

    // Emit scalar declarations
    for (const auto& s : func.scalars) {
        emit_scalar_decl(ctx, s);
    }

    ctx.emit("");

    // Emit operations
    for (const auto& op : func.ops) {
        emit_operation(ctx, *op);
    }

    ctx.pop_indent();
    ctx.emit("}");
    ctx.emit("");

    // Emit kernel registration
    emit_kernel_registration(ctx, func);
}

void AscendEmitter::emit_load(codegen::CodeGenContext& ctx, const ir::LoadOp& op) {
    codegen::Template tpl("DataCopy(${dst}, ${src}[${offset}], ${size});");
    tpl.set("dst", op.dst_tile)
       .set("src", op.src_memref)
       .set("offset", (int64_t)(op.row_offset * 128 + op.col_offset))
       .set("size", (int64_t)1);

    if (op.async && !op.tag.empty()) {
        ctx.emit("// Async load with tag: " + op.tag);
        tpl = codegen::Template("DataCopyAsync(${dst}, ${src}[${offset}], ${size}, ${tag});");
        tpl.set("dst", op.dst_tile)
           .set("src", op.src_memref)
           .set("offset", (int64_t)(op.row_offset * 128 + op.col_offset))
           .set("size", (int64_t)1)
           .set("tag", "EVENT_" + op.tag);
    }

    ctx.emit(tpl.render());
}

void AscendEmitter::emit_store(codegen::CodeGenContext& ctx, const ir::StoreOp& op) {
    codegen::Template tpl("DataCopy(${dst}[${offset}], ${src}, ${size});");
    tpl.set("dst", op.dst_memref)
       .set("src", op.src_tile)
       .set("offset", (int64_t)(op.row_offset * 128 + op.col_offset))
       .set("size", (int64_t)1);

    if (op.async && !op.tag.empty()) {
        tpl = codegen::Template("DataCopyAsync(${dst}[${offset}], ${src}, ${size}, ${tag});");
        tpl.set("dst", op.dst_memref)
           .set("src", op.src_tile)
           .set("offset", (int64_t)(op.row_offset * 128 + op.col_offset))
           .set("size", (int64_t)1)
           .set("tag", "EVENT_" + op.tag);
    }

    ctx.emit(tpl.render());
}

void AscendEmitter::emit_binary(codegen::CodeGenContext& ctx, const ir::BinaryOp& op) {
    std::string op_func;
    switch (op.kind) {
        case ir::NPUOpKind::Add: op_func = "Add"; break;
        case ir::NPUOpKind::Mul: op_func = "Mul"; break;
        case ir::NPUOpKind::Sub: op_func = "Sub"; break;
        case ir::NPUOpKind::Div: op_func = "Div"; break;
        default: op_func = "BinaryOp"; break;
    }

    codegen::Template tpl("${op}(${dst}, ${a}, ${b});");
    tpl.set("op", op_func)
       .set("dst", op.dst)
       .set("a", op.a)
       .set("b", op.b);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_unary(codegen::CodeGenContext& ctx, const ir::UnaryOp& op) {
    std::string op_func;
    switch (op.kind) {
        case ir::NPUOpKind::Exp: op_func = "Exp"; break;
        case ir::NPUOpKind::Rsqrt: op_func = "Rsqrt"; break;
        case ir::NPUOpKind::Neg: op_func = "Neg"; break;
        case ir::NPUOpKind::Abs: op_func = "Abs"; break;
        default: op_func = "UnaryOp"; break;
    }

    codegen::Template tpl("${op}(${dst}, ${src});");
    tpl.set("op", op_func)
       .set("dst", op.dst)
       .set("src", op.src);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_reduce(codegen::CodeGenContext& ctx, const ir::ReduceOp& op) {
    std::string op_func;
    switch (op.kind) {
        case ir::NPUOpKind::RowSum: op_func = "RowSum"; break;
        case ir::NPUOpKind::RowMax: op_func = "RowMax"; break;
        case ir::NPUOpKind::RowMin: op_func = "RowMin"; break;
        case ir::NPUOpKind::RowMean: op_func = "RowMean"; break;
        default: op_func = "ReduceOp"; break;
    }

    codegen::Template tpl("Reduce<${op}>(${dst}, ${src});");
    tpl.set("op", op_func)
       .set("dst", op.dst)
       .set("src", op.src);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_broadcast(codegen::CodeGenContext& ctx, const ir::BroadcastOp& op) {
    std::string op_func;
    switch (op.kind) {
        case ir::NPUOpKind::RowExpandMul: op_func = "RowExpandMul"; break;
        case ir::NPUOpKind::RowExpandAdd: op_func = "RowExpandAdd"; break;
        case ir::NPUOpKind::RowExpandSub: op_func = "RowExpandSub"; break;
        default: op_func = "BroadcastOp"; break;
    }

    codegen::Template tpl("Broadcast<${op}>(${dst}, ${a}, ${b});");
    tpl.set("op", op_func)
       .set("dst", op.dst)
       .set("a", op.a)
       .set("b", op.b);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_matmul(codegen::CodeGenContext& ctx, const ir::MatmulOp& op) {
    if (op.use_cube) {
        codegen::Template tpl("Cube::Matmul(${dst}, ${a}, ${b});");
        tpl.set("dst", op.dst)
           .set("a", op.a)
           .set("b", op.b);
        ctx.emit(tpl.render());
    } else {
        codegen::Template tpl("Vector::Matmul(${dst}, ${a}, ${b});");
        tpl.set("dst", op.dst)
           .set("a", op.a)
           .set("b", op.b);
        ctx.emit(tpl.render());
    }
}

void AscendEmitter::emit_for_begin(codegen::CodeGenContext& ctx, const ir::ForLoopBeginOp& op) {
    codegen::Template tpl("for (int ${iv} = ${lb}; ${iv} < ${ub}; ${iv} += ${step}) {");
    tpl.set("iv", op.iv)
       .set("lb", op.lb)
       .set("ub", op.ub)
       .set("step", op.step);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_for_end(codegen::CodeGenContext& ctx) {
    ctx.emit("}");
}

void AscendEmitter::emit_wait(codegen::CodeGenContext& ctx, const ir::WaitOp& op) {
    codegen::Template tpl("WaitEvent(EVENT_${tag});");
    tpl.set("tag", op.tag);
    ctx.emit(tpl.render());
}

// Private helper methods

void AscendEmitter::emit_tile_decl(codegen::CodeGenContext& ctx, const ir::TileDecl& t) {
    std::string dtype_str = dtype_to_cann(t.dtype);
    std::string loc_str = location_to_cann(t.location);

    codegen::Template tpl("Tile<${dtype}, ${rows}, ${cols}, ${loc}> ${name};");
    tpl.set("dtype", dtype_str)
       .set("rows", t.rows)
       .set("cols", t.cols)
       .set("loc", loc_str)
       .set("name", t.name);
    ctx.emit(tpl.render());
}

void AscendEmitter::emit_scalar_decl(codegen::CodeGenContext& ctx, const ir::ScalarDecl& s) {
    std::string dtype_str = dtype_to_cann(s.dtype);
    std::ostringstream oss;
    oss << dtype_str << " " << s.name;
    if (s.immediate) {
        oss << " = ";
        std::visit([&oss](auto v) { oss << v; }, *s.immediate);
    }
    oss << ";";
    ctx.emit(oss.str());
}

void AscendEmitter::emit_operation(codegen::CodeGenContext& ctx, const ir::NPUOp& op) {
    switch (op.kind) {
        case ir::NPUOpKind::Load:
            emit_load(ctx, static_cast<const ir::LoadOp&>(op));
            break;
        case ir::NPUOpKind::Store:
            emit_store(ctx, static_cast<const ir::StoreOp&>(op));
            break;
        case ir::NPUOpKind::Add:
        case ir::NPUOpKind::Mul:
        case ir::NPUOpKind::Sub:
        case ir::NPUOpKind::Div:
            emit_binary(ctx, static_cast<const ir::BinaryOp&>(op));
            break;
        case ir::NPUOpKind::Exp:
        case ir::NPUOpKind::Rsqrt:
        case ir::NPUOpKind::Neg:
        case ir::NPUOpKind::Abs:
            emit_unary(ctx, static_cast<const ir::UnaryOp&>(op));
            break;
        case ir::NPUOpKind::RowSum:
        case ir::NPUOpKind::RowMax:
        case ir::NPUOpKind::RowMin:
        case ir::NPUOpKind::RowMean:
            emit_reduce(ctx, static_cast<const ir::ReduceOp&>(op));
            break;
        case ir::NPUOpKind::RowExpandMul:
        case ir::NPUOpKind::RowExpandAdd:
        case ir::NPUOpKind::RowExpandSub:
            emit_broadcast(ctx, static_cast<const ir::BroadcastOp&>(op));
            break;
        case ir::NPUOpKind::Matmul:
            emit_matmul(ctx, static_cast<const ir::MatmulOp&>(op));
            break;
        case ir::NPUOpKind::ForLoopBegin:
            emit_for_begin(ctx, static_cast<const ir::ForLoopBeginOp&>(op));
            ctx.push_indent();
            break;
        case ir::NPUOpKind::ForLoopEnd:
            ctx.pop_indent();
            emit_for_end(ctx);
            break;
        case ir::NPUOpKind::Wait:
            emit_wait(ctx, static_cast<const ir::WaitOp&>(op));
            break;
        default:
            ctx.emit_comment("Unhandled operation");
            break;
    }
}

void AscendEmitter::emit_kernel_registration(codegen::CodeGenContext& ctx, const ir::NPUFunction& func) {
    ctx.emit("// Register kernel with runtime");
    codegen::Template tpl("REGISTER_KERNEL(\"${name}\", ${name});");
    tpl.set("name", func.name);
    ctx.emit(tpl.render());
    ctx.emit("");
}

std::string AscendEmitter::dtype_to_cann(ir::DType dtype) {
    switch (dtype) {
        case ir::DType::F16: return "half";
        case ir::DType::BF16: return "bfloat16";
        case ir::DType::F32: return "float";
        case ir::DType::F64: return "double";
        case ir::DType::I8: return "int8_t";
        case ir::DType::I16: return "int16_t";
        case ir::DType::I32: return "int32_t";
        case ir::DType::I64: return "int64_t";
        case ir::DType::U8: return "uint8_t";
        case ir::DType::U16: return "uint16_t";
        case ir::DType::U32: return "uint32_t";
        case ir::DType::U64: return "uint64_t";
        case ir::DType::Bool: return "bool";
        default: return "void";
    }
}

std::string AscendEmitter::location_to_cann(ir::Location loc) {
    switch (loc) {
        case ir::Location::Global: return "GM";
        case ir::Location::L2: return "L2";
        case ir::Location::UB: return "UB";
        case ir::Location::L1: return "L1";
        default: return "GM";
    }
}

// ============================================================
// AscendNPUProgram Implementation
// ============================================================

AscendNPUProgram::AscendNPUProgram(graph::TaskGraphStorage storage,
                                   ScheduleRuntimeConfig config,
                                   std::string code)
    : storage_(std::move(storage)),
      config_(std::move(config)),
      generated_code_(std::move(code)) {}

void AscendNPUProgram::execute() {
    throw std::runtime_error("Ascend NPU execution requires CANN runtime");
}

void AscendNPUProgram::execute_async() {
    throw std::runtime_error("Ascend NPU execution requires CANN runtime");
}

void AscendNPUProgram::synchronize() {}

bool AscendNPUProgram::is_complete() const { return false; }

double AscendNPUProgram::elapsed_ms() const { return 0; }

ProgramStats AscendNPUProgram::stats() const {
    return ProgramStats{
        .num_tasks = storage_.num_tasks(),
        .num_streams = config_.num_streams,
        .num_executors = 0,
        .compile_time_ms = 0,
        .execute_time_ms = 0,
        .peak_memory_bytes = 0,
        .total_edges = storage_.num_edges(),
    };
}

std::string AscendNPUProgram::dump() const {
    return "AscendNPUProgram (codegen mode)\n\nGenerated code:\n" + generated_code_;
}

const std::string& AscendNPUProgram::generated_code() const { return generated_code_; }

// ============================================================
// NPU Workload Lowering
// ============================================================

/// Helper class that walks workload IR and generates NPU tasks.
/// Similar to CPU lowerer but registers tasks with AscendAICore domain.
class NPUWorkloadLowerer {
public:
    NPUWorkloadLowerer(graph::TaskGraphStorage& storage, const ir::Module& module)
        : storage_(storage), builder_(storage) {
        // Register all kernels from workload definitions
        for (const auto& workload : module.workloads) {
            if (workload.body) {
                register_kernels(workload.body);
            }
        }
    }

    void lower(const ir::Module& module) {
        for (const auto& workload : module.workloads) {
            if (workload.body) {
                std::unordered_map<std::string, int64_t> bindings;
                lower_workload(workload.body, bindings);
            }
        }
        builder_.finalize();
    }

private:
    graph::TaskGraphStorage& storage_;
    graph::TaskGraphBuilder builder_;
    std::vector<graph::TaskId> last_tasks_;

    void register_kernels(const ir::IRPtr<ir::WorkloadNode>& node) {
        if (!node) return;

        if (node->kind == ir::NodeKind::Task) {
            auto task = std::static_pointer_cast<const ir::TaskNode>(node);
            graph::KernelInfo info{
                task->kernel_name, task->kernel_name, 0, 0,
                graph::ExecDomain::AscendAICore, graph::ExecPool::Any
            };
            storage_.kernel_bundle().register_kernel(info);
        }

        node->forEachChild([this](const ir::IRPtr<ir::IRNode>& child) {
            auto workload = std::dynamic_pointer_cast<const ir::WorkloadNode>(child);
            if (workload) {
                register_kernels(workload);
            }
        });
    }

    void lower_workload(const ir::IRPtr<ir::WorkloadNode>& node,
                        std::unordered_map<std::string, int64_t>& bindings) {
        if (!node) return;

        switch (node->kind) {
            case ir::NodeKind::Task:
                lower_task(std::static_pointer_cast<const ir::TaskNode>(node), bindings);
                break;
            case ir::NodeKind::ParallelFor:
                lower_parallel_for(
                    std::static_pointer_cast<const ir::ParallelForNode>(node), bindings);
                break;
            case ir::NodeKind::ForEach:
                lower_for_each(
                    std::static_pointer_cast<const ir::ForEachNode>(node), bindings);
                break;
            case ir::NodeKind::Combine:
                lower_combine(
                    std::static_pointer_cast<const ir::CombineNode>(node), bindings);
                break;
            case ir::NodeKind::Sequential:
                lower_sequential(
                    std::static_pointer_cast<const ir::SequentialNode>(node), bindings);
                break;
            default:
                break;
        }
    }

    void lower_task(const ir::IRPtr<ir::TaskNode>& task,
                    const std::unordered_map<std::string, int64_t>& bindings) {
        auto kernel_id = storage_.kernel_bundle().find_kernel(task->kernel_name);
        if (!kernel_id) return;

        builder_.begin_task(*kernel_id)
                .set_domain(graph::ExecDomain::AscendAICore);

        for (const auto& param : task->params) {
            auto it = bindings.find(param);
            if (it != bindings.end()) {
                builder_.add_arg(static_cast<uint64_t>(it->second));
            }
        }

        graph::TaskId tid = builder_.submit();
        last_tasks_.push_back(tid);
    }

    void lower_parallel_for(const ir::IRPtr<ir::ParallelForNode>& pf,
                            std::unordered_map<std::string, int64_t>& bindings) {
        int64_t size = get_axis_size(pf->axis);
        if (size <= 0) return;

        for (int64_t i = 0; i < size; ++i) {
            bindings[pf->index_var] = i;
            lower_workload(pf->body, bindings);
        }
        bindings.erase(pf->index_var);
    }

    void lower_for_each(const ir::IRPtr<ir::ForEachNode>& fe,
                        std::unordered_map<std::string, int64_t>& bindings) {
        int64_t size = get_axis_size(fe->axis);
        if (size <= 0) return;

        graph::TaskId prev_task = graph::INVALID_TASK_ID;
        for (int64_t i = 0; i < size; ++i) {
            bindings[fe->index_var] = i;
            size_t before = last_tasks_.size();
            lower_workload(fe->body, bindings);
            size_t after = last_tasks_.size();

            if (prev_task != graph::INVALID_TASK_ID && after > before) {
                for (size_t j = before; j < after; ++j) {
                    builder_.add_dependency(prev_task, last_tasks_[j]);
                }
            }
            if (after > before) {
                prev_task = last_tasks_[after - 1];
            }
        }
        bindings.erase(fe->index_var);
    }

    void lower_combine(const ir::IRPtr<ir::CombineNode>& combine,
                       std::unordered_map<std::string, int64_t>& bindings) {
        for (const auto& child : combine->workloads) {
            lower_workload(child, bindings);
        }
    }

    void lower_sequential(const ir::IRPtr<ir::SequentialNode>& seq,
                          std::unordered_map<std::string, int64_t>& bindings) {
        std::vector<graph::TaskId> prev_tasks;

        for (const auto& child : seq->workloads) {
            size_t before = last_tasks_.size();
            lower_workload(child, bindings);
            size_t after = last_tasks_.size();

            if (!prev_tasks.empty() && after > before) {
                for (graph::TaskId prev : prev_tasks) {
                    for (size_t j = before; j < after; ++j) {
                        builder_.add_dependency(prev, last_tasks_[j]);
                    }
                }
            }

            prev_tasks.clear();
            for (size_t j = before; j < after; ++j) {
                prev_tasks.push_back(last_tasks_[j]);
            }
        }
    }

    /// Get size from axis, or -1 if dynamic without runtime context
    ///
    /// For static Dense axes, returns the compile-time size.
    /// For dynamic axes (DenseDyn, Ragged, Sparse), returns -1 to indicate
    /// that runtime expansion is needed.
    int64_t get_axis_size(const ir::IRPtr<ir::AxisNode>& axis) {
        if (!axis) return -1;

        switch (axis->kind) {
            case ir::NodeKind::DenseAxis: {
                auto dense = std::static_pointer_cast<const ir::DenseAxisNode>(axis);
                return dense->size;
            }

            case ir::NodeKind::DenseDynAxis:
            case ir::NodeKind::RaggedAxis:
            case ir::NodeKind::SparseAxis:
                // Dynamic axes require runtime size lookup
                // Return -1 to indicate dynamic expansion needed
                return -1;

            default:
                return -1;
        }
    }
};

// ============================================================
// AscendNPUBackend Implementation
// ============================================================

AscendNPUBackend::AscendNPUBackend() : emitter_(std::make_unique<AscendEmitter>()) {}

std::string AscendNPUBackend::name() const { return "ascend_npu"; }

std::vector<std::string> AscendNPUBackend::supported_targets() const {
    return {"ascend_npu", "ascend", "npu"};
}

bool AscendNPUBackend::supports(const ir::Module& module) const {
    // Support modules with NPU-level workloads
    for (const auto& w : module.workloads) {
        if (w.level == ir::WorkloadLevel::NPU) {
            return true;
        }
    }
    return false;
}

bool AscendNPUBackend::supports(ir::NodeKind kind) const {
    // Support NPU-related node kinds
    switch (kind) {
        case ir::NodeKind::Task:
        case ir::NodeKind::ParallelFor:
        case ir::NodeKind::ForEach:
        case ir::NodeKind::Dispatch:
        case ir::NodeKind::Stream:
        case ir::NodeKind::Timing:
        case ir::NodeKind::PipelineDepth:
        case ir::NodeKind::TaskWindow:
        case ir::NodeKind::BatchDeps:
            return true;
        default:
            return false;
    }
}

/// Lower IR Module to a LoweredPlan for Ascend NPU execution.
///
/// Transforms workload IR into a task graph configured for NPU execution.
/// The Ascend backend uses dual-queue scheduling to separate AI Core tasks
/// from host coordination tasks.
///
/// Key differences from CPU backend:
/// - dual_queue_enabled = true for separate NPU/host queues
/// - Kernels registered with AscendAICore domain
/// - Schedule config tuned for NPU memory hierarchy
///
/// @param module The IR module with workload definitions
/// @param options Compile options (window size, pipeline depth, streams)
/// @return LoweredPlan ready for NPU program compilation
///
/// @note Static Dense axes are fully expanded. NPU-specific scheduling
///       (spatial mapping, layouts) is applied via schedule directives.
LoweredPlan AscendNPUBackend::lower(const ir::Module& module, const CompileOptions& options) {
    LoweredPlan plan;
    plan.workload_name = module.name;

    // Configure from options
    plan.sched_config.window_size = options.task_window_size;
    plan.sched_config.window_mode = options.task_window_mode;
    plan.sched_config.pipeline_depth = options.pipeline_depth;
    plan.sched_config.gate_scope = options.gate_scope;
    plan.sched_config.batch_threshold = options.batch_deps_threshold;
    plan.sched_config.num_streams = options.num_streams;
    plan.sched_config.dual_queue_enabled = true;  // NPU uses dual queue

    // Use NPU lowerer to expand IR into concrete tasks
    NPUWorkloadLowerer lowerer(plan.graph, module);
    lowerer.lower(module);

    return plan;
}

/// Compile a LoweredPlan into an AscendNPUProgram.
///
/// Unlike CPU backend which creates an immediately executable program,
/// the NPU backend produces a program that contains generated CANN code.
/// This code must be compiled by the CANN toolchain before execution.
///
/// @param plan The lowered plan with NPU task graph
/// @param options Compile options (unused for codegen backend)
/// @return Program containing generated CANN source code
std::unique_ptr<Program> AscendNPUBackend::compile(const LoweredPlan& plan,
                                                    const CompileOptions& /*options*/) {
    // Create a placeholder program that stores the generated code
    auto program = std::make_unique<AscendNPUProgram>(
        plan.graph, plan.sched_config, generated_code_);
    return program;
}

/// Generate CANN-compatible C++ code from an NPU module.
///
/// Uses the template-based CodeGenerator with AscendEmitter to produce
/// source code that can be compiled by the CANN toolchain. The generated
/// code includes:
/// - Tile operation implementations using CANN intrinsics
/// - Memory management (Global Memory â†” Local Memory transfers)
/// - Kernel launch wrappers
///
/// @param module The NPU module containing kernel and function definitions
/// @return Generated C++ source code string
std::string AscendNPUBackend::generate_code(const ir::NPUModule& module) {
    codegen::CodeGenerator gen(std::make_unique<AscendEmitter>());
    generated_code_ = gen.generate(module);
    return generated_code_;
}

const std::string& AscendNPUBackend::get_generated_code() const { return generated_code_; }

}  // namespace pto::wsp::backend::ascend
